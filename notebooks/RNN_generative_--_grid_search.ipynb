{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c285edb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../code/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e23c5973",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from rnn import RNN\n",
    "from runner import Runner\n",
    "import pandas as pd\n",
    "from utils import invert_dict, load_lm_dataset, docs_to_indices, seqs_to_lmXY\n",
    "from rnnmath import fraq_loss\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2a588e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = '../data/'\n",
    "np.random.seed(2018)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7ee89e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rates = [0.5, 0.1, 0.05]\n",
    "hdims = [25, 50]\n",
    "lookbacks = [0, 2, 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d112ecc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = 1000\n",
    "dev_size = 1000\n",
    "vocab_size = 2000\n",
    "epochs=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e57aafa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retained 2000 words from 9954 (88.35% of all tokens)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# get the data set vocabulary\n",
    "vocab = pd.read_table(data_folder + \"/vocab.wiki.txt\", header=None, sep=\"\\s+\", index_col=0,\n",
    "                      names=['count', 'freq'], )\n",
    "num_to_word = dict(enumerate(vocab.index[:vocab_size]))\n",
    "word_to_num = invert_dict(num_to_word)\n",
    "\n",
    "# calculate loss vocabulary words due to vocab_size\n",
    "fraction_lost = fraq_loss(vocab, word_to_num, vocab_size)\n",
    "print(\n",
    "    \"Retained %d words from %d (%.02f%% of all tokens)\\n\" % (\n",
    "    vocab_size, len(vocab), 100 * (1 - fraction_lost)))\n",
    "\n",
    "docs = load_lm_dataset(data_folder + '/wiki-train.txt')\n",
    "S_train = docs_to_indices(docs, word_to_num, 1, 1)\n",
    "X_train, D_train = seqs_to_lmXY(S_train)\n",
    "\n",
    "# Load the dev set (for tuning hyperparameters)\n",
    "docs = load_lm_dataset(data_folder + '/wiki-dev.txt')\n",
    "S_dev = docs_to_indices(docs, word_to_num, 1, 1)\n",
    "X_dev, D_dev = seqs_to_lmXY(S_dev)\n",
    "\n",
    "X_train = X_train[:train_size]\n",
    "D_train = D_train[:train_size]\n",
    "X_dev = X_dev[:dev_size]\n",
    "D_dev = D_dev[:dev_size]\n",
    "\n",
    "# q = best unigram frequency from omitted vocab\n",
    "# this is the best expected loss out of that set\n",
    "q = vocab.freq[vocab_size] / sum(vocab.freq[vocab_size:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fade0a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = [learning_rates, hdims, lookbacks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e6b22c41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training model for 10 epochs\n",
      "training set: 1000 sentences (batch size 100)\n",
      "Optimizing loss on 1000 sentences\n",
      "Vocab size: 2000\n",
      "Hidden units: 25\n",
      "Steps for back propagation: 0\n",
      "Initial learning rate set to 0.5, annealing set to 5\n",
      "\n",
      "calculating initial mean loss on dev set: 7.798662515757492\n",
      "\n",
      "epoch 1, learning rate 0.5000\tinstance 1000\tepoch done in 38.54 seconds\tnew loss: 8.15014653696414\n",
      "epoch 2, learning rate 0.4167\tinstance 1000\tepoch done in 38.02 seconds\tnew loss: 5.978210046559437\n",
      "epoch 3, learning rate 0.3571\tinstance 1000\tepoch done in 38.40 seconds\tnew loss: 5.746970466401593\n",
      "epoch 4, learning rate 0.3125\tinstance 379"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/p3/ls8tlrt101n8mnp7jb_v1m_40000gn/T/ipykernel_13991/1033530074.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mback_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlookback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     )\n",
      "\u001b[0;32m~/PycharmProjects/NLP_CW1/code/runner.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, X, D, X_dev, D_dev, epochs, learning_rate, anneal, back_steps, batch_size, min_change, log)\u001b[0m\n\u001b[1;32m    200\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0my_p\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms_p\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_p\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 202\u001b[0;31m                 \u001b[0;32mif\u001b[0m \u001b[0mback_steps\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    203\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macc_deltas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_p\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_p\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_p\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms_p\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PycharmProjects/NLP_CW1/code/rnn.py\u001b[0m in \u001b[0;36macc_deltas\u001b[0;34m(self, x, d, y, s)\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m             \u001b[0msigmoid_derivative\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m             \u001b[0mdelta_in_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mdelta_out_t\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0msigmoid_derivative\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m             \u001b[0;31m# wrt V\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Q2.a\n",
    "\n",
    "# !!! I had to change the train function to make this display in jupyter notebook, which is not allowed\n",
    "for lr, hdim, lookback in itertools.product(*s):\n",
    "    r = Runner(model=RNN(vocab_size=vocab_size, hidden_dims=hdim, out_vocab_size=vocab_size))\n",
    "    r.train(\n",
    "        X=X_train,\n",
    "        D=D_train,\n",
    "        X_dev=X_dev,\n",
    "        D_dev=D_dev,\n",
    "        epochs=epochs,\n",
    "        learning_rate=lr,\n",
    "        back_steps=lookback\n",
    "    )\n",
    "    \n",
    "    print('######################################################################')\n",
    "    print('######################################################################')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "479d7cb0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlu",
   "language": "python",
   "name": "nlu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
