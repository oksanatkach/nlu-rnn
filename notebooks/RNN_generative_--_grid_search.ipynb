{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c285edb9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../code/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e23c5973",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from rnn import RNN\n",
    "from runner import Runner\n",
    "import pandas as pd\n",
    "from utils import invert_dict, load_lm_dataset, docs_to_indices, seqs_to_lmXY\n",
    "from rnnmath import fraq_loss\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2a588e30",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_folder = '../data/'\n",
    "np.random.seed(2018)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7ee89e21",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "learning_rates = [0.5, 0.1, 0.05]\n",
    "hdims = [25, 50]\n",
    "lookbacks = [0, 2, 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d112ecc8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_size = 1000\n",
    "dev_size = 1000\n",
    "vocab_size = 2000\n",
    "epochs=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e57aafa7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retained 2000 words from 9954 (88.35% of all tokens)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# get the data set vocabulary\n",
    "vocab = pd.read_table(data_folder + \"/vocab.wiki.txt\", header=None, sep=\"\\s+\", index_col=0,\n",
    "                      names=['count', 'freq'], )\n",
    "num_to_word = dict(enumerate(vocab.index[:vocab_size]))\n",
    "word_to_num = invert_dict(num_to_word)\n",
    "\n",
    "# calculate loss vocabulary words due to vocab_size\n",
    "fraction_lost = fraq_loss(vocab, word_to_num, vocab_size)\n",
    "print(\n",
    "    \"Retained %d words from %d (%.02f%% of all tokens)\\n\" % (\n",
    "    vocab_size, len(vocab), 100 * (1 - fraction_lost)))\n",
    "\n",
    "docs = load_lm_dataset(data_folder + '/wiki-train.txt')\n",
    "S_train = docs_to_indices(docs, word_to_num, 1, 1)\n",
    "X_train, D_train = seqs_to_lmXY(S_train)\n",
    "\n",
    "# Load the dev set (for tuning hyperparameters)\n",
    "docs = load_lm_dataset(data_folder + '/wiki-dev.txt')\n",
    "S_dev = docs_to_indices(docs, word_to_num, 1, 1)\n",
    "X_dev, D_dev = seqs_to_lmXY(S_dev)\n",
    "\n",
    "X_train = X_train[:train_size]\n",
    "D_train = D_train[:train_size]\n",
    "X_dev = X_dev[:dev_size]\n",
    "D_dev = D_dev[:dev_size]\n",
    "\n",
    "# q = best unigram frequency from omitted vocab\n",
    "# this is the best expected loss out of that set\n",
    "q = vocab.freq[vocab_size] / sum(vocab.freq[vocab_size:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fade0a21",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "s = [learning_rates, hdims, lookbacks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e6b22c41",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training model for 10 epochs\n",
      "training set: 1000 sentences (batch size 100)\n",
      "Optimizing loss on 1000 sentences\n",
      "Vocab size: 2000\n",
      "Hidden units: 25\n",
      "Steps for back propagation: 0\n",
      "Initial learning rate set to 0.5, annealing set to 5\n",
      "\n",
      "calculating initial mean loss on dev set: 7.798662515757492\n",
      "\n",
      "epoch 1, learning rate 0.5000\tinstance 1000\tepoch done in 36.99 seconds\tnew loss: 8.15014653696414\n",
      "epoch 2, learning rate 0.4167\tinstance 1000\tepoch done in 36.75 seconds\tnew loss: 5.978210046559437\n",
      "epoch 3, learning rate 0.3571\tinstance 1000\tepoch done in 35.57 seconds\tnew loss: 5.746970466401593\n",
      "epoch 4, learning rate 0.3125\tinstance 1000\tepoch done in 47.49 seconds\tnew loss: 5.195948023144267\n",
      "epoch 5, learning rate 0.2778\tinstance 1000\tepoch done in 36.97 seconds\tnew loss: 5.154120129486588\n",
      "epoch 6, learning rate 0.2500\tinstance 1000\tepoch done in 37.59 seconds\tnew loss: 5.11580921789912\n",
      "epoch 7, learning rate 0.2273\tinstance 1000\tepoch done in 37.26 seconds\tnew loss: 5.114597758027232\n",
      "epoch 8, learning rate 0.2083\tinstance 1000\tepoch done in 37.41 seconds\tnew loss: 5.04692175607107\n",
      "epoch 9, learning rate 0.1923\tinstance 1000\tepoch done in 38.02 seconds\tnew loss: 5.029777551548084\n",
      "epoch 10, learning rate 0.1786\tinstance 1000\tepoch done in 36.35 seconds\tnew loss: 5.016975861837498\n",
      "\n",
      "training finished after reaching maximum of 10 epochs\n",
      "best observed loss was 5.016975861837498, at epoch 10\n",
      "setting parameters to matrices from best epoch\n",
      "######################################################################\n",
      "######################################################################\n",
      "\n",
      "Training model for 10 epochs\n",
      "training set: 1000 sentences (batch size 100)\n",
      "Optimizing loss on 1000 sentences\n",
      "Vocab size: 2000\n",
      "Hidden units: 25\n",
      "Steps for back propagation: 2\n",
      "Initial learning rate set to 0.5, annealing set to 5\n",
      "\n",
      "calculating initial mean loss on dev set: 8.122025716192606\n",
      "\n",
      "epoch 1, learning rate 0.5000\tinstance 1000\tepoch done in 41.91 seconds\tnew loss: 7.697708658550071\n",
      "epoch 2, learning rate 0.4167\tinstance 1000\tepoch done in 40.96 seconds\tnew loss: 5.874172013018087\n",
      "epoch 3, learning rate 0.3571\tinstance 1000\tepoch done in 38.02 seconds\tnew loss: 5.3812477486265395\n",
      "epoch 4, learning rate 0.3125\tinstance 1000\tepoch done in 37.09 seconds\tnew loss: 5.130588072723059\n",
      "epoch 5, learning rate 0.2778\tinstance 1000\tepoch done in 37.25 seconds\tnew loss: 5.095481254671179\n",
      "epoch 6, learning rate 0.2500\tinstance 1000\tepoch done in 37.78 seconds\tnew loss: 5.047728996453035\n",
      "epoch 7, learning rate 0.2273\tinstance 1000\tepoch done in 37.71 seconds\tnew loss: 5.0204814488637295\n",
      "epoch 8, learning rate 0.2083\tinstance 1000\tepoch done in 39.16 seconds\tnew loss: 5.00495205926863\n",
      "epoch 9, learning rate 0.1923\tinstance 1000\tepoch done in 38.48 seconds\tnew loss: 4.994626440019565\n",
      "epoch 10, learning rate 0.1786\tinstance 1000\tepoch done in 38.06 seconds\tnew loss: 4.9780692329758685\n",
      "\n",
      "training finished after reaching maximum of 10 epochs\n",
      "best observed loss was 4.9780692329758685, at epoch 10\n",
      "setting parameters to matrices from best epoch\n",
      "######################################################################\n",
      "######################################################################\n",
      "\n",
      "Training model for 10 epochs\n",
      "training set: 1000 sentences (batch size 100)\n",
      "Optimizing loss on 1000 sentences\n",
      "Vocab size: 2000\n",
      "Hidden units: 25\n",
      "Steps for back propagation: 5\n",
      "Initial learning rate set to 0.5, annealing set to 5\n",
      "\n",
      "calculating initial mean loss on dev set: 8.005974516119998\n",
      "\n",
      "epoch 1, learning rate 0.5000\tinstance 1000\tepoch done in 44.89 seconds\tnew loss: 5.449484495800915\n",
      "epoch 2, learning rate 0.4167\tinstance 1000\tepoch done in 43.71 seconds\tnew loss: 5.198745729082761\n",
      "epoch 3, learning rate 0.3571\tinstance 1000\tepoch done in 43.03 seconds\tnew loss: 5.164632800897689\n",
      "epoch 4, learning rate 0.3125\tinstance 1000\tepoch done in 43.33 seconds\tnew loss: 5.116082361334575\n",
      "epoch 5, learning rate 0.2778\tinstance 1000\tepoch done in 43.76 seconds\tnew loss: 5.013985997112231\n",
      "epoch 6, learning rate 0.2500\tinstance 1000\tepoch done in 42.46 seconds\tnew loss: 5.0049480104054815\n",
      "epoch 7, learning rate 0.2273\tinstance 1000\tepoch done in 48.73 seconds\tnew loss: 4.973612734611133\n",
      "epoch 8, learning rate 0.2083\tinstance 1000\tepoch done in 46.11 seconds\tnew loss: 4.960081625501704\n",
      "epoch 9, learning rate 0.1923\tinstance 1000\tepoch done in 43.71 seconds\tnew loss: 4.968205054304816\n",
      "epoch 10, learning rate 0.1786\tinstance 1000\tepoch done in 43.40 seconds\tnew loss: 4.933136297644741\n",
      "\n",
      "training finished after reaching maximum of 10 epochs\n",
      "best observed loss was 4.933136297644741, at epoch 10\n",
      "setting parameters to matrices from best epoch\n",
      "######################################################################\n",
      "######################################################################\n",
      "\n",
      "Training model for 10 epochs\n",
      "training set: 1000 sentences (batch size 100)\n",
      "Optimizing loss on 1000 sentences\n",
      "Vocab size: 2000\n",
      "Hidden units: 50\n",
      "Steps for back propagation: 0\n",
      "Initial learning rate set to 0.5, annealing set to 5\n",
      "\n",
      "calculating initial mean loss on dev set: 8.356998486898831\n",
      "\n",
      "epoch 1, learning rate 0.5000\tinstance 1000\tepoch done in 39.95 seconds\tnew loss: 7.414658445440394\n",
      "epoch 2, learning rate 0.4167\tinstance 1000\tepoch done in 40.25 seconds\tnew loss: 5.56986478933479\n",
      "epoch 3, learning rate 0.3571\tinstance 1000\tepoch done in 41.95 seconds\tnew loss: 5.210865684902249\n",
      "epoch 4, learning rate 0.3125\tinstance 1000\tepoch done in 40.08 seconds\tnew loss: 5.103003967137481\n",
      "epoch 5, learning rate 0.2778\tinstance 1000\tepoch done in 42.08 seconds\tnew loss: 5.3990675569845425\n",
      "epoch 6, learning rate 0.2500\tinstance 1000\tepoch done in 39.88 seconds\tnew loss: 5.027687152438031\n",
      "epoch 7, learning rate 0.2273\tinstance 1000\tepoch done in 40.50 seconds\tnew loss: 4.9990688525307485\n",
      "epoch 8, learning rate 0.2083\tinstance 1000\tepoch done in 43.84 seconds\tnew loss: 4.985172738596687\n",
      "epoch 9, learning rate 0.1923\tinstance 1000\tepoch done in 59.47 seconds\tnew loss: 4.968243366673972\n",
      "epoch 10, learning rate 0.1786\tinstance 1000\tepoch done in 53.46 seconds\tnew loss: 4.970246293925321\n",
      "\n",
      "training finished after reaching maximum of 10 epochs\n",
      "best observed loss was 4.968243366673972, at epoch 9\n",
      "setting parameters to matrices from best epoch\n",
      "######################################################################\n",
      "######################################################################\n",
      "\n",
      "Training model for 10 epochs\n",
      "training set: 1000 sentences (batch size 100)\n",
      "Optimizing loss on 1000 sentences\n",
      "Vocab size: 2000\n",
      "Hidden units: 50\n",
      "Steps for back propagation: 2\n",
      "Initial learning rate set to 0.5, annealing set to 5\n",
      "\n",
      "calculating initial mean loss on dev set: 8.159437732854677\n",
      "\n",
      "epoch 1, learning rate 0.5000\tinstance 1000\tepoch done in 580.74 seconds\tnew loss: 8.384125840782248\n",
      "epoch 2, learning rate 0.4167\tinstance 1000\tepoch done in 45.59 seconds\tnew loss: 6.5512549395525825\n",
      "epoch 3, learning rate 0.3571\tinstance 1000\tepoch done in 44.60 seconds\tnew loss: 5.820566850936603\n",
      "epoch 4, learning rate 0.3125\tinstance 1000\tepoch done in 46.92 seconds\tnew loss: 5.510317297996779\n",
      "epoch 5, learning rate 0.2778\tinstance 1000\tepoch done in 44.98 seconds\tnew loss: 5.212303382183007\n",
      "epoch 6, learning rate 0.2500\tinstance 1000\tepoch done in 44.93 seconds\tnew loss: 5.293023062628523\n",
      "epoch 7, learning rate 0.2273\tinstance 1000\tepoch done in 43.46 seconds\tnew loss: 5.071218116875907\n",
      "epoch 8, learning rate 0.2083\tinstance 1000\tepoch done in 42.62 seconds\tnew loss: 5.068860972990066\n",
      "epoch 9, learning rate 0.1923\tinstance 1000\tepoch done in 45.38 seconds\tnew loss: 5.035908041475284\n",
      "epoch 10, learning rate 0.1786\tinstance 1000\tepoch done in 46.88 seconds\tnew loss: 5.027570601193062\n",
      "\n",
      "training finished after reaching maximum of 10 epochs\n",
      "best observed loss was 5.027570601193062, at epoch 10\n",
      "setting parameters to matrices from best epoch\n",
      "######################################################################\n",
      "######################################################################\n",
      "\n",
      "Training model for 10 epochs\n",
      "training set: 1000 sentences (batch size 100)\n",
      "Optimizing loss on 1000 sentences\n",
      "Vocab size: 2000\n",
      "Hidden units: 50\n",
      "Steps for back propagation: 5\n",
      "Initial learning rate set to 0.5, annealing set to 5\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating initial mean loss on dev set: 8.623476566919857\n",
      "\n",
      "epoch 1, learning rate 0.5000\tinstance 1000\tepoch done in 49.50 seconds\tnew loss: 7.512639839083601\n",
      "epoch 2, learning rate 0.4167\tinstance 1000\tepoch done in 50.38 seconds\tnew loss: 6.097188652709496\n",
      "epoch 3, learning rate 0.3571\tinstance 1000\tepoch done in 50.59 seconds\tnew loss: 5.232716284227191\n",
      "epoch 4, learning rate 0.3125\tinstance 1000\tepoch done in 50.82 seconds\tnew loss: 5.109902864772836\n",
      "epoch 5, learning rate 0.2778\tinstance 1000\tepoch done in 52.94 seconds\tnew loss: 5.099857759180985\n",
      "epoch 6, learning rate 0.2500\tinstance 1000\tepoch done in 53.21 seconds\tnew loss: 5.0543468434561705\n",
      "epoch 7, learning rate 0.2273\tinstance 1000\tepoch done in 54.28 seconds\tnew loss: 5.016407418581707\n",
      "epoch 8, learning rate 0.2083\tinstance 1000\tepoch done in 50.74 seconds\tnew loss: 4.995120137679506\n",
      "epoch 9, learning rate 0.1923\tinstance 1000\tepoch done in 53.76 seconds\tnew loss: 4.978735220242406\n",
      "epoch 10, learning rate 0.1786\tinstance 1000\tepoch done in 49.08 seconds\tnew loss: 4.964726004087516\n",
      "\n",
      "training finished after reaching maximum of 10 epochs\n",
      "best observed loss was 4.964726004087516, at epoch 10\n",
      "setting parameters to matrices from best epoch\n",
      "######################################################################\n",
      "######################################################################\n",
      "\n",
      "Training model for 10 epochs\n",
      "training set: 1000 sentences (batch size 100)\n",
      "Optimizing loss on 1000 sentences\n",
      "Vocab size: 2000\n",
      "Hidden units: 25\n",
      "Steps for back propagation: 0\n",
      "Initial learning rate set to 0.1, annealing set to 5\n",
      "\n",
      "calculating initial mean loss on dev set: 7.899192285877461\n",
      "\n",
      "epoch 1, learning rate 0.1000\tinstance 1000\tepoch done in 33.76 seconds\tnew loss: 5.777144924874238\n",
      "epoch 2, learning rate 0.0833\tinstance 1000\tepoch done in 33.62 seconds\tnew loss: 5.535528956995092\n",
      "epoch 3, learning rate 0.0714\tinstance 1000\tepoch done in 33.39 seconds\tnew loss: 5.433896499634995\n",
      "epoch 4, learning rate 0.0625\tinstance 1000\tepoch done in 33.25 seconds\tnew loss: 5.380447450873842\n",
      "epoch 5, learning rate 0.0556\tinstance 1000\tepoch done in 32.63 seconds\tnew loss: 5.324680510629727\n",
      "epoch 6, learning rate 0.0500\tinstance 1000\tepoch done in 34.05 seconds\tnew loss: 5.289558195250442\n",
      "epoch 7, learning rate 0.0455\tinstance 1000\tepoch done in 34.54 seconds\tnew loss: 5.260198585765353\n",
      "epoch 8, learning rate 0.0417\tinstance 1000\tepoch done in 33.43 seconds\tnew loss: 5.236725772880232\n",
      "epoch 9, learning rate 0.0385\tinstance 1000\tepoch done in 34.19 seconds\tnew loss: 5.214215013583637\n",
      "epoch 10, learning rate 0.0357\tinstance 1000\tepoch done in 37.24 seconds\tnew loss: 5.195890896884486\n",
      "\n",
      "training finished after reaching maximum of 10 epochs\n",
      "best observed loss was 5.195890896884486, at epoch 10\n",
      "setting parameters to matrices from best epoch\n",
      "######################################################################\n",
      "######################################################################\n",
      "\n",
      "Training model for 10 epochs\n",
      "training set: 1000 sentences (batch size 100)\n",
      "Optimizing loss on 1000 sentences\n",
      "Vocab size: 2000\n",
      "Hidden units: 25\n",
      "Steps for back propagation: 2\n",
      "Initial learning rate set to 0.1, annealing set to 5\n",
      "\n",
      "calculating initial mean loss on dev set: 7.9166664024617655\n",
      "\n",
      "epoch 1, learning rate 0.1000\tinstance 1000\tepoch done in 40.71 seconds\tnew loss: 5.837014830649938\n",
      "epoch 2, learning rate 0.0833\tinstance 1000\tepoch done in 39.50 seconds\tnew loss: 5.534050025747921\n",
      "epoch 3, learning rate 0.0714\tinstance 1000\tepoch done in 40.90 seconds\tnew loss: 5.427819483331139\n",
      "epoch 4, learning rate 0.0625\tinstance 1000\tepoch done in 40.04 seconds\tnew loss: 5.363452438839512\n",
      "epoch 5, learning rate 0.0556\tinstance 1000\tepoch done in 43.75 seconds\tnew loss: 5.318469608561559\n",
      "epoch 6, learning rate 0.0500\tinstance 1000\tepoch done in 49.81 seconds\tnew loss: 5.283908658276463\n",
      "epoch 7, learning rate 0.0455\tinstance 1000\tepoch done in 44.48 seconds\tnew loss: 5.254786936040149\n",
      "epoch 8, learning rate 0.0417\tinstance 1000\tepoch done in 45.15 seconds\tnew loss: 5.230270284969352\n",
      "epoch 9, learning rate 0.0385\tinstance 1000\tepoch done in 45.36 seconds\tnew loss: 5.209179777191631\n",
      "epoch 10, learning rate 0.0357\tinstance 1000\tepoch done in 44.30 seconds\tnew loss: 5.190748163295863\n",
      "\n",
      "training finished after reaching maximum of 10 epochs\n",
      "best observed loss was 5.190748163295863, at epoch 10\n",
      "setting parameters to matrices from best epoch\n",
      "######################################################################\n",
      "######################################################################\n",
      "\n",
      "Training model for 10 epochs\n",
      "training set: 1000 sentences (batch size 100)\n",
      "Optimizing loss on 1000 sentences\n",
      "Vocab size: 2000\n",
      "Hidden units: 25\n",
      "Steps for back propagation: 5\n",
      "Initial learning rate set to 0.1, annealing set to 5\n",
      "\n",
      "calculating initial mean loss on dev set: 8.220088265486238\n",
      "\n",
      "epoch 1, learning rate 0.1000\tinstance 1000\tepoch done in 49.21 seconds\tnew loss: 5.939081795999758\n",
      "epoch 2, learning rate 0.0833\tinstance 1000\tepoch done in 48.80 seconds\tnew loss: 5.5541421635139185\n",
      "epoch 3, learning rate 0.0714\tinstance 1000\tepoch done in 48.35 seconds\tnew loss: 5.451240552450753\n",
      "epoch 4, learning rate 0.0625\tinstance 1000\tepoch done in 48.09 seconds\tnew loss: 5.388170133775631\n",
      "epoch 5, learning rate 0.0556\tinstance 1000\tepoch done in 48.10 seconds\tnew loss: 5.341397484595131\n",
      "epoch 6, learning rate 0.0500\tinstance 1000\tepoch done in 49.92 seconds\tnew loss: 5.305676083767191\n",
      "epoch 7, learning rate 0.0455\tinstance 1000\tepoch done in 48.49 seconds\tnew loss: 5.276648710876227\n",
      "epoch 8, learning rate 0.0417\tinstance 1000\tepoch done in 48.85 seconds\tnew loss: 5.250897739309842\n",
      "epoch 9, learning rate 0.0385\tinstance 1000\tepoch done in 44.25 seconds\tnew loss: 5.228683813810095\n",
      "epoch 10, learning rate 0.0357\tinstance 1000\tepoch done in 40.28 seconds\tnew loss: 5.209637875094277\n",
      "\n",
      "training finished after reaching maximum of 10 epochs\n",
      "best observed loss was 5.209637875094277, at epoch 10\n",
      "setting parameters to matrices from best epoch\n",
      "######################################################################\n",
      "######################################################################\n",
      "\n",
      "Training model for 10 epochs\n",
      "training set: 1000 sentences (batch size 100)\n",
      "Optimizing loss on 1000 sentences\n",
      "Vocab size: 2000\n",
      "Hidden units: 50\n",
      "Steps for back propagation: 0\n",
      "Initial learning rate set to 0.1, annealing set to 5\n",
      "\n",
      "calculating initial mean loss on dev set: 8.529788725926974\n",
      "\n",
      "epoch 1, learning rate 0.1000\tinstance 1000\tepoch done in 36.48 seconds\tnew loss: 5.943425109943794\n",
      "epoch 2, learning rate 0.0833\tinstance 1000\tepoch done in 36.32 seconds\tnew loss: 5.506663540728915\n",
      "epoch 3, learning rate 0.0714\tinstance 1000\tepoch done in 38.38 seconds\tnew loss: 5.415174649084886\n",
      "epoch 4, learning rate 0.0625\tinstance 1000\tepoch done in 41.35 seconds\tnew loss: 5.330311601945388\n",
      "epoch 5, learning rate 0.0556\tinstance 1000\tepoch done in 40.86 seconds\tnew loss: 5.284552743098903\n",
      "epoch 6, learning rate 0.0500\tinstance 1000\tepoch done in 39.08 seconds\tnew loss: 5.248513922885052\n",
      "epoch 7, learning rate 0.0455\tinstance 1000\tepoch done in 41.67 seconds\tnew loss: 5.217101997505155\n",
      "epoch 8, learning rate 0.0417\tinstance 1000\tepoch done in 37.94 seconds\tnew loss: 5.191705336598912\n",
      "epoch 9, learning rate 0.0385\tinstance 1000\tepoch done in 47.94 seconds\tnew loss: 5.170826504895868\n",
      "epoch 10, learning rate 0.0357\tinstance 1000\tepoch done in 53.76 seconds\tnew loss: 5.1522680032401045\n",
      "\n",
      "training finished after reaching maximum of 10 epochs\n",
      "best observed loss was 5.1522680032401045, at epoch 10\n",
      "setting parameters to matrices from best epoch\n",
      "######################################################################\n",
      "######################################################################\n",
      "\n",
      "Training model for 10 epochs\n",
      "training set: 1000 sentences (batch size 100)\n",
      "Optimizing loss on 1000 sentences\n",
      "Vocab size: 2000\n",
      "Hidden units: 50\n",
      "Steps for back propagation: 2\n",
      "Initial learning rate set to 0.1, annealing set to 5\n",
      "\n",
      "calculating initial mean loss on dev set: 8.622490634921743\n",
      "\n",
      "epoch 1, learning rate 0.1000\tinstance 1000\tepoch done in 59.25 seconds\tnew loss: 6.3190838847212145\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2, learning rate 0.0833\tinstance 1000\tepoch done in 72.50 seconds\tnew loss: 5.4651961552558985\n",
      "epoch 3, learning rate 0.0714\tinstance 1000\tepoch done in 66.47 seconds\tnew loss: 5.351179945648193\n",
      "epoch 4, learning rate 0.0625\tinstance 1000\tepoch done in 64.32 seconds\tnew loss: 5.288592158274152\n",
      "epoch 5, learning rate 0.0556\tinstance 1000\tepoch done in 64.18 seconds\tnew loss: 5.244431551949272\n",
      "epoch 6, learning rate 0.0500\tinstance 1000\tepoch done in 63.26 seconds\tnew loss: 5.210745937843555\n",
      "epoch 7, learning rate 0.0455\tinstance 1000\tepoch done in 64.81 seconds\tnew loss: 5.184193722531978\n",
      "epoch 8, learning rate 0.0417\tinstance 1000\tepoch done in 65.12 seconds\tnew loss: 5.1601219727632035\n",
      "epoch 9, learning rate 0.0385\tinstance 1000\tepoch done in 64.67 seconds\tnew loss: 5.140233154322142\n",
      "epoch 10, learning rate 0.0357\tinstance 1000\tepoch done in 66.57 seconds\tnew loss: 5.123840002858845\n",
      "\n",
      "training finished after reaching maximum of 10 epochs\n",
      "best observed loss was 5.123840002858845, at epoch 10\n",
      "setting parameters to matrices from best epoch\n",
      "######################################################################\n",
      "######################################################################\n",
      "\n",
      "Training model for 10 epochs\n",
      "training set: 1000 sentences (batch size 100)\n",
      "Optimizing loss on 1000 sentences\n",
      "Vocab size: 2000\n",
      "Hidden units: 50\n",
      "Steps for back propagation: 5\n",
      "Initial learning rate set to 0.1, annealing set to 5\n",
      "\n",
      "calculating initial mean loss on dev set: 8.713143605402111\n",
      "\n",
      "epoch 1, learning rate 0.1000\tinstance 1000\tepoch done in 72.95 seconds\tnew loss: 5.707490810721253\n",
      "epoch 2, learning rate 0.0833\tinstance 1000\tepoch done in 76.17 seconds\tnew loss: 5.4826503400009345\n",
      "epoch 3, learning rate 0.0714\tinstance 1000\tepoch done in 88.51 seconds\tnew loss: 5.382057433089496\n",
      "epoch 4, learning rate 0.0625\tinstance 1000\tepoch done in 75.52 seconds\tnew loss: 5.319984381195008\n",
      "epoch 5, learning rate 0.0556\tinstance 1000\tepoch done in 72.95 seconds\tnew loss: 5.272598052619201\n",
      "epoch 6, learning rate 0.0500\tinstance 1000\tepoch done in 57.25 seconds\tnew loss: 5.235928576976777\n",
      "epoch 7, learning rate 0.0455\tinstance 1000\tepoch done in 52.20 seconds\tnew loss: 5.205101815166097\n",
      "epoch 8, learning rate 0.0417\tinstance 1000\tepoch done in 52.77 seconds\tnew loss: 5.180857013538234\n",
      "epoch 9, learning rate 0.0385\tinstance 1000\tepoch done in 53.06 seconds\tnew loss: 5.159369665318631\n",
      "epoch 10, learning rate 0.0357\tinstance 1000\tepoch done in 52.34 seconds\tnew loss: 5.140867883246269\n",
      "\n",
      "training finished after reaching maximum of 10 epochs\n",
      "best observed loss was 5.140867883246269, at epoch 10\n",
      "setting parameters to matrices from best epoch\n",
      "######################################################################\n",
      "######################################################################\n",
      "\n",
      "Training model for 10 epochs\n",
      "training set: 1000 sentences (batch size 100)\n",
      "Optimizing loss on 1000 sentences\n",
      "Vocab size: 2000\n",
      "Hidden units: 25\n",
      "Steps for back propagation: 0\n",
      "Initial learning rate set to 0.05, annealing set to 5\n",
      "\n",
      "calculating initial mean loss on dev set: 7.7982082154310515\n",
      "\n",
      "epoch 1, learning rate 0.0500\tinstance 1000\tepoch done in 37.09 seconds\tnew loss: 6.238352371786259\n",
      "epoch 2, learning rate 0.0417\tinstance 1000\tepoch done in 36.42 seconds\tnew loss: 5.77518259410392\n",
      "epoch 3, learning rate 0.0357\tinstance 1000\tepoch done in 36.36 seconds\tnew loss: 5.594344547977013\n",
      "epoch 4, learning rate 0.0312\tinstance 1000\tepoch done in 35.07 seconds\tnew loss: 5.518851211964928\n",
      "epoch 5, learning rate 0.0278\tinstance 1000\tepoch done in 35.79 seconds\tnew loss: 5.474506022992276\n",
      "epoch 6, learning rate 0.0250\tinstance 1000\tepoch done in 36.36 seconds\tnew loss: 5.441935736870713\n",
      "epoch 7, learning rate 0.0227\tinstance 1000\tepoch done in 37.08 seconds\tnew loss: 5.415606902874581\n",
      "epoch 8, learning rate 0.0208\tinstance 1000\tepoch done in 38.27 seconds\tnew loss: 5.3940244813488025\n",
      "epoch 9, learning rate 0.0192\tinstance 1000\tepoch done in 37.53 seconds\tnew loss: 5.375811488598642\n",
      "epoch 10, learning rate 0.0179\tinstance 1000\tepoch done in 35.99 seconds\tnew loss: 5.3594378954376625\n",
      "\n",
      "training finished after reaching maximum of 10 epochs\n",
      "best observed loss was 5.3594378954376625, at epoch 10\n",
      "setting parameters to matrices from best epoch\n",
      "######################################################################\n",
      "######################################################################\n",
      "\n",
      "Training model for 10 epochs\n",
      "training set: 1000 sentences (batch size 100)\n",
      "Optimizing loss on 1000 sentences\n",
      "Vocab size: 2000\n",
      "Hidden units: 25\n",
      "Steps for back propagation: 2\n",
      "Initial learning rate set to 0.05, annealing set to 5\n",
      "\n",
      "calculating initial mean loss on dev set: 8.173202780610817\n",
      "\n",
      "epoch 1, learning rate 0.0500\tinstance 1000\tepoch done in 40.73 seconds\tnew loss: 6.2034452749070725\n",
      "epoch 2, learning rate 0.0417\tinstance 1000\tepoch done in 39.04 seconds\tnew loss: 5.740010110110972\n",
      "epoch 3, learning rate 0.0357\tinstance 1000\tepoch done in 38.14 seconds\tnew loss: 5.610302424133665\n",
      "epoch 4, learning rate 0.0312\tinstance 1000\tepoch done in 38.33 seconds\tnew loss: 5.544934797310444\n",
      "epoch 5, learning rate 0.0278\tinstance 1000\tepoch done in 37.46 seconds\tnew loss: 5.499441691716861\n",
      "epoch 6, learning rate 0.0250\tinstance 1000\tepoch done in 39.68 seconds\tnew loss: 5.465574755578675\n",
      "epoch 7, learning rate 0.0227\tinstance 1000\tepoch done in 40.33 seconds\tnew loss: 5.439091637657618\n",
      "epoch 8, learning rate 0.0208\tinstance 1000\tepoch done in 40.70 seconds\tnew loss: 5.4162741914562975\n",
      "epoch 9, learning rate 0.0192\tinstance 1000\tepoch done in 36.89 seconds\tnew loss: 5.397440839412028\n",
      "epoch 10, learning rate 0.0179\tinstance 1000\tepoch done in 40.10 seconds\tnew loss: 5.3808138917272945\n",
      "\n",
      "training finished after reaching maximum of 10 epochs\n",
      "best observed loss was 5.3808138917272945, at epoch 10\n",
      "setting parameters to matrices from best epoch\n",
      "######################################################################\n",
      "######################################################################\n",
      "\n",
      "Training model for 10 epochs\n",
      "training set: 1000 sentences (batch size 100)\n",
      "Optimizing loss on 1000 sentences\n",
      "Vocab size: 2000\n",
      "Hidden units: 25\n",
      "Steps for back propagation: 5\n",
      "Initial learning rate set to 0.05, annealing set to 5\n",
      "\n",
      "calculating initial mean loss on dev set: 7.975848348526116\n",
      "\n",
      "epoch 1, learning rate 0.0500\tinstance 1000\tepoch done in 44.00 seconds\tnew loss: 6.236278142419296\n",
      "epoch 2, learning rate 0.0417\tinstance 1000\tepoch done in 44.53 seconds\tnew loss: 5.7821832680719165\n",
      "epoch 3, learning rate 0.0357\tinstance 1000\tepoch done in 43.78 seconds\tnew loss: 5.620869982950247\n",
      "epoch 4, learning rate 0.0312\tinstance 1000\tepoch done in 43.50 seconds\tnew loss: 5.54228233785709\n",
      "epoch 5, learning rate 0.0278\tinstance 1000\tepoch done in 43.33 seconds\tnew loss: 5.494356929324896\n",
      "epoch 6, learning rate 0.0250\tinstance 1000\tepoch done in 43.74 seconds\tnew loss: 5.458775499913881\n",
      "epoch 7, learning rate 0.0227\tinstance 1000\tepoch done in 42.29 seconds\tnew loss: 5.431707747538839\n",
      "epoch 8, learning rate 0.0208\tinstance 1000\tepoch done in 43.95 seconds\tnew loss: 5.408627400349114\n",
      "epoch 9, learning rate 0.0192\tinstance 1000\tepoch done in 54.25 seconds\tnew loss: 5.389825700069598\n",
      "epoch 10, learning rate 0.0179\tinstance 1000\tepoch done in 42.77 seconds\tnew loss: 5.3730651479172025\n",
      "\n",
      "training finished after reaching maximum of 10 epochs\n",
      "best observed loss was 5.3730651479172025, at epoch 10\n",
      "setting parameters to matrices from best epoch\n",
      "######################################################################\n",
      "######################################################################\n",
      "\n",
      "Training model for 10 epochs\n",
      "training set: 1000 sentences (batch size 100)\n",
      "Optimizing loss on 1000 sentences\n",
      "Vocab size: 2000\n",
      "Hidden units: 50\n",
      "Steps for back propagation: 0\n",
      "Initial learning rate set to 0.05, annealing set to 5\n",
      "\n",
      "calculating initial mean loss on dev set: 7.9278225747898095\n",
      "\n",
      "epoch 1, learning rate 0.0500\tinstance 1000\tepoch done in 38.56 seconds\tnew loss: 5.94090205535138\n",
      "epoch 2, learning rate 0.0417\tinstance 1000\tepoch done in 39.23 seconds\tnew loss: 5.602166467059012\n",
      "epoch 3, learning rate 0.0357\tinstance 1000\tepoch done in 40.79 seconds\tnew loss: 5.486052139394797\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 4, learning rate 0.0312\tinstance 1000\tepoch done in 41.14 seconds\tnew loss: 5.424441192651989\n",
      "epoch 5, learning rate 0.0278\tinstance 1000\tepoch done in 38.29 seconds\tnew loss: 5.381836157626986\n",
      "epoch 6, learning rate 0.0250\tinstance 1000\tepoch done in 38.32 seconds\tnew loss: 5.349032180742898\n",
      "epoch 7, learning rate 0.0227\tinstance 1000\tepoch done in 38.96 seconds\tnew loss: 5.322961468400411\n",
      "epoch 8, learning rate 0.0208\tinstance 1000\tepoch done in 38.87 seconds\tnew loss: 5.301185899650594\n",
      "epoch 9, learning rate 0.0192\tinstance 1000\tepoch done in 39.17 seconds\tnew loss: 5.282966962502312\n",
      "epoch 10, learning rate 0.0179\tinstance 1000\tepoch done in 39.40 seconds\tnew loss: 5.267207273043814\n",
      "\n",
      "training finished after reaching maximum of 10 epochs\n",
      "best observed loss was 5.267207273043814, at epoch 10\n",
      "setting parameters to matrices from best epoch\n",
      "######################################################################\n",
      "######################################################################\n",
      "\n",
      "Training model for 10 epochs\n",
      "training set: 1000 sentences (batch size 100)\n",
      "Optimizing loss on 1000 sentences\n",
      "Vocab size: 2000\n",
      "Hidden units: 50\n",
      "Steps for back propagation: 2\n",
      "Initial learning rate set to 0.05, annealing set to 5\n",
      "\n",
      "calculating initial mean loss on dev set: 8.497008344064579\n",
      "\n",
      "epoch 1, learning rate 0.0500\tinstance 1000\tepoch done in 46.88 seconds\tnew loss: 5.8645415123641715\n",
      "epoch 2, learning rate 0.0417\tinstance 1000\tepoch done in 44.91 seconds\tnew loss: 5.6020263836990205\n",
      "epoch 3, learning rate 0.0357\tinstance 1000\tepoch done in 43.17 seconds\tnew loss: 5.504592362063118\n",
      "epoch 4, learning rate 0.0312\tinstance 1000\tepoch done in 45.20 seconds\tnew loss: 5.444712114208768\n",
      "epoch 5, learning rate 0.0278\tinstance 1000\tepoch done in 46.01 seconds\tnew loss: 5.402287558827438\n",
      "epoch 6, learning rate 0.0250\tinstance 1000\tepoch done in 44.78 seconds\tnew loss: 5.370080691739339\n",
      "epoch 7, learning rate 0.0227\tinstance 1000\tepoch done in 43.78 seconds\tnew loss: 5.343596523670612\n",
      "epoch 8, learning rate 0.0208\tinstance 1000\tepoch done in 42.73 seconds\tnew loss: 5.320978322949617\n",
      "epoch 9, learning rate 0.0192\tinstance 1000\tepoch done in 44.71 seconds\tnew loss: 5.3020521640567395\n",
      "epoch 10, learning rate 0.0179\tinstance 1000\tepoch done in 46.25 seconds\tnew loss: 5.2854174896154555\n",
      "\n",
      "training finished after reaching maximum of 10 epochs\n",
      "best observed loss was 5.2854174896154555, at epoch 10\n",
      "setting parameters to matrices from best epoch\n",
      "######################################################################\n",
      "######################################################################\n",
      "\n",
      "Training model for 10 epochs\n",
      "training set: 1000 sentences (batch size 100)\n",
      "Optimizing loss on 1000 sentences\n",
      "Vocab size: 2000\n",
      "Hidden units: 50\n",
      "Steps for back propagation: 5\n",
      "Initial learning rate set to 0.05, annealing set to 5\n",
      "\n",
      "calculating initial mean loss on dev set: 8.07173846999181\n",
      "\n",
      "epoch 1, learning rate 0.0500\tinstance 1000\tepoch done in 52.63 seconds\tnew loss: 5.975188832975169\n",
      "epoch 2, learning rate 0.0417\tinstance 1000\tepoch done in 52.56 seconds\tnew loss: 5.664230372082029\n",
      "epoch 3, learning rate 0.0357\tinstance 1000\tepoch done in 52.82 seconds\tnew loss: 5.553720599103529\n",
      "epoch 4, learning rate 0.0312\tinstance 1000\tepoch done in 53.78 seconds\tnew loss: 5.488633801103792\n",
      "epoch 5, learning rate 0.0278\tinstance 1000\tepoch done in 58.03 seconds\tnew loss: 5.443761042115766\n",
      "epoch 6, learning rate 0.0250\tinstance 1000\tepoch done in 55.54 seconds\tnew loss: 5.407800787808108\n",
      "epoch 7, learning rate 0.0227\tinstance 1000\tepoch done in 52.90 seconds\tnew loss: 5.378914385209316\n",
      "epoch 8, learning rate 0.0208\tinstance 1000\tepoch done in 51.98 seconds\tnew loss: 5.354977374276236\n",
      "epoch 9, learning rate 0.0192\tinstance 1000\tepoch done in 52.22 seconds\tnew loss: 5.334003811396482\n",
      "epoch 10, learning rate 0.0179\tinstance 1000\tepoch done in 52.57 seconds\tnew loss: 5.31625329379674\n",
      "\n",
      "training finished after reaching maximum of 10 epochs\n",
      "best observed loss was 5.31625329379674, at epoch 10\n",
      "setting parameters to matrices from best epoch\n",
      "######################################################################\n",
      "######################################################################\n"
     ]
    }
   ],
   "source": [
    "# Q2.a\n",
    "\n",
    "# !!! I had to change the train function to make this display in jupyter notebook, which is not allowed\n",
    "for lr, hdim, lookback in itertools.product(*s):\n",
    "    r = Runner(model=RNN(vocab_size=vocab_size, hidden_dims=hdim, out_vocab_size=vocab_size))\n",
    "    r.train(\n",
    "        X=X_train,\n",
    "        D=D_train,\n",
    "        X_dev=X_dev,\n",
    "        D_dev=D_dev,\n",
    "        epochs=epochs,\n",
    "        learning_rate=lr,\n",
    "        back_steps=lookback\n",
    "    )\n",
    "    \n",
    "    print('######################################################################')\n",
    "    print('######################################################################')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "479d7cb0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlu",
   "language": "python",
   "name": "nlu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
