{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6bdabdac",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../code/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a7cbd276",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from gru import GRU\n",
    "from runner import Runner\n",
    "import pandas as pd\n",
    "from utils import invert_dict, load_np_dataset, docs_to_indices, seqs_to_npXY\n",
    "from rnnmath import fraq_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "be434395",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_folder = '../data/'\n",
    "np.random.seed(2018)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "64be2846",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_size = 2000\n",
    "dev_size = 1000 # ??\n",
    "vocab_size = 2000\n",
    "epochs=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bbab72e5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "hdims = [10, 25, 50]\n",
    "lr = 0.5\n",
    "lookback = 0 # ??\n",
    "out_vocab_size = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "18898494",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retained 2000 words from 9954 (88.35% of all tokens)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vocab = pd.read_table(data_folder + \"/vocab.wiki.txt\", header=None, sep=\"\\s+\", index_col=0,\n",
    "                      names=['count', 'freq'], )\n",
    "num_to_word = dict(enumerate(vocab.index[:vocab_size]))\n",
    "word_to_num = invert_dict(num_to_word)\n",
    "\n",
    "# calculate loss vocabulary words due to vocab_size\n",
    "fraction_lost = fraq_loss(vocab, word_to_num, vocab_size)\n",
    "print(\n",
    "    \"Retained %d words from %d (%.02f%% of all tokens)\\n\" % (\n",
    "    vocab_size, len(vocab), 100 * (1 - fraction_lost)))\n",
    "\n",
    "# load training data\n",
    "sents = load_np_dataset(data_folder + '/wiki-train.txt')\n",
    "S_train = docs_to_indices(sents, word_to_num, 0, 0)\n",
    "X_train, D_train = seqs_to_npXY(S_train)\n",
    "\n",
    "X_train = X_train[:train_size]\n",
    "Y_train = D_train[:train_size]\n",
    "\n",
    "# load development data\n",
    "sents = load_np_dataset(data_folder + '/wiki-dev.txt')\n",
    "S_dev = docs_to_indices(sents, word_to_num, 0, 0)\n",
    "X_dev, D_dev = seqs_to_npXY(S_dev)\n",
    "\n",
    "X_dev = X_dev[:dev_size]\n",
    "D_dev = D_dev[:dev_size]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "82867fe5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training model for 10 epochs\n",
      "training set: 2000 sentences (batch size 100)\n",
      "Optimizing loss on 1000 sentences\n",
      "Vocab size: 2000\n",
      "Hidden units: 10\n",
      "Steps for back propagation: 0\n",
      "Initial learning rate set to 0.5, annealing set to 5\n",
      "\n",
      "calculating initial mean loss on dev set: 0.6957307846288112\n",
      "calculating initial acc on dev set: 0.511\n",
      "\n",
      "epoch 1, learning rate 0.5000\tinstance 2000\tepoch done in 30.70 seconds\tnew loss: 0.6742038563151017\tnew acc: 0.601\n",
      "epoch 2, learning rate 0.4167\tinstance 2000\tepoch done in 39.84 seconds\tnew loss: 0.6590071662506569\tnew acc: 0.628\n",
      "epoch 3, learning rate 0.3571\tinstance 2000\tepoch done in 32.63 seconds\tnew loss: 0.6470836368833998\tnew acc: 0.655\n",
      "epoch 4, learning rate 0.3125\tinstance 2000\tepoch done in 35.17 seconds\tnew loss: 0.6361240483134482\tnew acc: 0.659\n",
      "epoch 5, learning rate 0.2778\tinstance 2000\tepoch done in 30.03 seconds\tnew loss: 0.6265102334309524\tnew acc: 0.666\n",
      "epoch 6, learning rate 0.2500\tinstance 2000\tepoch done in 31.61 seconds\tnew loss: 0.6181509607813958\tnew acc: 0.663\n",
      "epoch 7, learning rate 0.2273\tinstance 2000\tepoch done in 31.08 seconds\tnew loss: 0.6099929028765292\tnew acc: 0.669\n",
      "epoch 8, learning rate 0.2083\tinstance 2000\tepoch done in 29.13 seconds\tnew loss: 0.6032668786136066\tnew acc: 0.667\n",
      "epoch 9, learning rate 0.1923\tinstance 2000\tepoch done in 31.47 seconds\tnew loss: 0.5971593287552928\tnew acc: 0.668\n",
      "epoch 10, learning rate 0.1786\tinstance 2000\tepoch done in 32.38 seconds\tnew loss: 0.591751258236688\tnew acc: 0.672\n",
      "\n",
      "training finished after reaching maximum of 10 epochs\n",
      "best observed loss was 0.591751258236688, acc 0.672, at epoch 10\n",
      "setting U, V, W to matrices from best epoch\n",
      "######################################################################\n",
      "######################################################################\n",
      "\n",
      "Training model for 10 epochs\n",
      "training set: 2000 sentences (batch size 100)\n",
      "Optimizing loss on 1000 sentences\n",
      "Vocab size: 2000\n",
      "Hidden units: 25\n",
      "Steps for back propagation: 0\n",
      "Initial learning rate set to 0.5, annealing set to 5\n",
      "\n",
      "calculating initial mean loss on dev set: 0.7113568713220445\n",
      "calculating initial acc on dev set: 0.454\n",
      "\n",
      "epoch 1, learning rate 0.5000\tinstance 2000\tepoch done in 32.99 seconds\tnew loss: 0.6552942807946804\tnew acc: 0.638\n",
      "epoch 2, learning rate 0.4167\tinstance 2000\tepoch done in 35.86 seconds\tnew loss: 0.6224418376172233\tnew acc: 0.689\n",
      "epoch 3, learning rate 0.3571\tinstance 2000\tepoch done in 31.23 seconds\tnew loss: 0.6012204179120081\tnew acc: 0.7\n",
      "epoch 4, learning rate 0.3125\tinstance 2000\tepoch done in 32.79 seconds\tnew loss: 0.5863895641843498\tnew acc: 0.705\n",
      "epoch 5, learning rate 0.2778\tinstance 2000\tepoch done in 33.49 seconds\tnew loss: 0.5753081361816721\tnew acc: 0.71\n",
      "epoch 6, learning rate 0.2500\tinstance 2000\tepoch done in 32.65 seconds\tnew loss: 0.5673204148130759\tnew acc: 0.711\n",
      "epoch 7, learning rate 0.2273\tinstance 2000\tepoch done in 32.72 seconds\tnew loss: 0.5611067708373243\tnew acc: 0.718\n",
      "epoch 8, learning rate 0.2083\tinstance 2000\tepoch done in 35.40 seconds\tnew loss: 0.5564253684803122\tnew acc: 0.717\n",
      "epoch 9, learning rate 0.1923\tinstance 2000\tepoch done in 31.64 seconds\tnew loss: 0.5523745331079378\tnew acc: 0.717\n",
      "epoch 10, learning rate 0.1786\tinstance 2000\tepoch done in 33.00 seconds\tnew loss: 0.549023691100592\tnew acc: 0.718\n",
      "\n",
      "training finished after reaching maximum of 10 epochs\n",
      "best observed loss was 0.549023691100592, acc 0.718, at epoch 10\n",
      "setting U, V, W to matrices from best epoch\n",
      "######################################################################\n",
      "######################################################################\n",
      "\n",
      "Training model for 10 epochs\n",
      "training set: 2000 sentences (batch size 100)\n",
      "Optimizing loss on 1000 sentences\n",
      "Vocab size: 2000\n",
      "Hidden units: 50\n",
      "Steps for back propagation: 0\n",
      "Initial learning rate set to 0.5, annealing set to 5\n",
      "\n",
      "calculating initial mean loss on dev set: 0.6447575978961468\n",
      "calculating initial acc on dev set: 0.629\n",
      "\n",
      "epoch 1, learning rate 0.5000\tinstance 2000\tepoch done in 34.53 seconds\tnew loss: 0.5896816533510315\tnew acc: 0.69\n",
      "epoch 2, learning rate 0.4167\tinstance 2000\tepoch done in 37.01 seconds\tnew loss: 0.5705668207084053\tnew acc: 0.71\n",
      "epoch 3, learning rate 0.3571\tinstance 2000\tepoch done in 34.72 seconds\tnew loss: 0.5593981834220941\tnew acc: 0.723\n",
      "epoch 4, learning rate 0.3125\tinstance 2000\tepoch done in 43.19 seconds\tnew loss: 0.5513180378032327\tnew acc: 0.728\n",
      "epoch 5, learning rate 0.2778\tinstance 2000\tepoch done in 37.39 seconds\tnew loss: 0.5449966644754062\tnew acc: 0.732\n",
      "epoch 6, learning rate 0.2500\tinstance 2000\tepoch done in 43.47 seconds\tnew loss: 0.54036024840784\tnew acc: 0.741\n",
      "epoch 7, learning rate 0.2273\tinstance 2000\tepoch done in 34.71 seconds\tnew loss: 0.5360118857364502\tnew acc: 0.746\n",
      "epoch 8, learning rate 0.2083\tinstance 2000\tepoch done in 33.27 seconds\tnew loss: 0.532742324075264\tnew acc: 0.754\n",
      "epoch 9, learning rate 0.1923\tinstance 2000\tepoch done in 33.41 seconds\tnew loss: 0.5299672248884615\tnew acc: 0.754\n",
      "epoch 10, learning rate 0.1786\tinstance 2000\tepoch done in 34.72 seconds\tnew loss: 0.5272272007576934\tnew acc: 0.757\n",
      "\n",
      "training finished after reaching maximum of 10 epochs\n",
      "best observed loss was 0.5272272007576934, acc 0.757, at epoch 10\n",
      "setting U, V, W to matrices from best epoch\n",
      "######################################################################\n",
      "######################################################################\n"
     ]
    }
   ],
   "source": [
    "# Q3.d\n",
    "\n",
    "for hdim in hdims:\n",
    "    r = Runner(model=GRU(vocab_size=vocab_size, hidden_dims=hdim, out_vocab_size=out_vocab_size))\n",
    "    r.train_np(\n",
    "        X=X_train,\n",
    "        D=D_train,\n",
    "        X_dev=X_dev,\n",
    "        D_dev=D_dev,\n",
    "        epochs=epochs,\n",
    "        learning_rate=lr,\n",
    "        back_steps=lookback\n",
    "    )\n",
    "    \n",
    "    print('######################################################################')\n",
    "    print('######################################################################')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeb40d01",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlu",
   "language": "python",
   "name": "nlu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
